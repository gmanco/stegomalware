import torch
import torch.nn as nn


class Conv2DBlock(nn.Module):
    def __init__(self, in_c, out_c, kernel, stride, padding):
        super(Conv2DBlock, self).__init__()

        self.block = nn.Sequential(
            nn.Conv2d(in_c, out_c, kernel, stride, padding),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Dropout(0.2),
            nn.BatchNorm2d(out_c)
        )

        self.init_weights()

    def init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):
                nn.init.kaiming_normal_(
                    m.weight, mode='fan_out', nonlinearity='relu')
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.Linear):
                nn.init.normal_(m.weight, 0, 0.01)
                nn.init.constant_(m.bias, 0)

    def forward(self,x):
        return self.block(x)


class TransConv2DBlock(nn.Module):
    def __init__(self, in_c, out_c, kernel, stride, padding, output_padding=1):
        super(TransConv2DBlock, self).__init__()

        self.block = nn.Sequential(
            nn.ConvTranspose2d(in_c, out_c, kernel, stride, padding, output_padding=output_padding),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Dropout(0.2),
            nn.BatchNorm2d(out_c)
        )

        self.init_weights()

    def init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):
                nn.init.kaiming_normal_(
                    m.weight, mode='fan_out', nonlinearity='relu')
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.Linear):
                nn.init.normal_(m.weight, 0, 0.01)
                nn.init.constant_(m.bias, 0)

    def forward(self, x):
        return self.block(x)



class AutoEncoder(nn.Module):
    def __init__(self, input_shape):
        super(AutoEncoder, self).__init__()


        self.conv1 = Conv2DBlock(input_shape, 64, (6,3), 1, (2,1))
        # 480 x 480
        self.conv2 = Conv2DBlock(64, 64, 3, 2, 1)
        # 240 x 240
        self.conv3 = Conv2DBlock(64, 128, 3, 2, 1)
        # 120 x 120
        self.conv4 = Conv2DBlock(128, 128, 3,1,1)
        # 120 x 120
        self.conv5 = Conv2DBlock(128, 256, 3,2,1)
        # 60 x 60
        self.conv6 = Conv2DBlock(256, 512, 3,2,1)
        # 30 x 30

        self.deconv1 = TransConv2DBlock(512, 256, 3, 2, 1)
        # 30 x 30
        self.conv7 = Conv2DBlock(512, 256, 3, 1, 1)
        # 60 x 60
        self.deconv2 = TransConv2DBlock(256, 128, 3, 2, 1)
        # 120 x 120
        self.conv8 = Conv2DBlock(256, 128, 3, 1, 1)
        # 120 x 120
        self.deconv3 = TransConv2DBlock(128, 64, 3, 2, 1)
        # 240 x 240
        self.conv9 = Conv2DBlock(128, 128, 3, 1, 1)
        self.deconv4 = TransConv2DBlock(128, 64, 3, 2, 1)
        # 480 x 480
        self.conv10 = Conv2DBlock(128, 64, 3, 1, 1)
        # 480 x 480
        self.deconv5 = TransConv2DBlock(64, 64, (6,3), 1, (2,1), output_padding=0)
        # 481 x 480
        self.conv11 = nn.Conv2d(64, 3, 3, 1, 1)
        # 481 x 480

    def forward(self, x):
        # 481 x 480 x 3
        x = self.conv1(x)
        # 480 x 480 x 64
        x1 = self.conv2(x)
        # 240 x 240 x 64
        x2 = self.conv3(x1)
        # 120 x 120 x 128
        x3 = self.conv4(x2)
        # 120 x 120 x 128
        x4 = self.conv5(x3)
        # 60 x 60 x 256
        x5 = self.conv6(x4)
        # 30 x 30 x 512

        y5 = self.deconv1(x5)
        # 60 x 60 x 256
        y5 = torch.cat((y5,x4),1)
        # 60 x 60 x 512
        y5 = self.conv7(y5)
        # 60 x 60 x 256
        y4 = self.deconv2(y5)
        # 120 x 120 x 128
        y4 = torch.cat((y4,x3),1)
        # 120 x 120 x 256
        y4 = self.conv8(y4)
        # 120 x 120 x 128
        y3 = self.deconv3(y4)
        # 240 x 240 x 64
        y3 = torch.cat((y3,x1),1)
        # 240 x 240 x 128
        y3 = self.conv9(y3)
        # 240 x 240 x 128
        y2 = self.deconv4(y3)
        # 480 x 480 x 64
        y2 = torch.cat((y2,x),1)
        # 480 x 480 x 128
        y2 = self.conv10(y2)
        # 480 x 480 x 64
        y1 = self.deconv5(y2)
        # 481 x 480 x 64
        y1 = self.conv11(y1)
        # 481 x 480 x 3

        y = torch.sigmoid(y1)

        return y


input = torch.randn(1,3,481,480)

model = AutoEncoder(3)

output = model(input)